# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**
this dataset contains data about NBA players, their repective age, thier playing position, their respective teams and their ratings in different domains. It also have their salary. Given the dataset we seet to predict player playing position given game features of respective player.

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**
As it is a classification task, we tried to solve it using hyperparameter tunning of logistic regression and using AutoML feature of Azure. using hyperparameter tunning we got the best accuracy of 91.1 with C (Inverse of regularization strength) of 0.51 and max_iter of 50. With AutoML we got the best accuracy of 94.87 with Voting Ensemble Algorithm.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

![alt text](https://github.com/Keshav-agrawal2829/Udacity-azureML/blob/main/pipeline.png)
Sklearn pipeline contained following steps:
- data is downloaded from "https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv" 
- data preprocessing - removing emtpy cells, converting categorical features to one hot encoding
- data split - splitting data into test and train data
- model hyperparameter - Inverse of regularization strength (C), Maximum number of iterations to converge( max_iters) 
- Classfication Algorithm - Logistic Regression


**What are the benefits of the parameter sampler you chose?**
Random Parameter Sampling offers a valuable tool for hyperparameter tuning in machine learning, particularly for its simplicity, efficiency, unbiased exploration, and ease of parallelization.

**What are the benefits of the early stopping policy you chose?**
It save the computing cost and early stopping also prevents from overfitting of the model.
## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
Using AutoML many ML models have been tested which is described in below image
![alt text](https://github.com/Keshav-agrawal2829/udacity-azureML/blob/main/AutoML_models_hyperparameters.PNG)

As Voting Ensemble method gives the best accuracy, it uses following Algorithms 
- 3 different hypterparameter settings of Standard Scaler with XGBoost with weightage of 0.066  
- 3 different hypterparamter setting of Max Abs Scaler with LightGBMClassifier with weight of 0.267, 0.133, 0.33
- SparseNormalizer with XGBoost with weightage of 0.066

Further Details of hyperparameters can be found on Automated ML section on Azure


## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
With Sklearn pipeline with hyperparameter tunning we got maximum accuary of 91.1 with logistic regression. and using Auzre AutoML we got maximum accuracy of 94.87 with Voting Ensemble Algorithm
## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
We can change the sampler such that grid sampling and stopping policy to get batter results.

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section. Image of cluster marked for deletion
